{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_875srK9_2I"
   },
   "source": [
    "<p>\n",
    "CAS on Advanced Machine Learning <br>\n",
    "Data Science Lab, University of Bern, 2026 <br>\n",
    "Prepared by Dr. Mykhailo Vladymyrov.\n",
    "\n",
    "</p>\n",
    "\n",
    "This work is licensed under a <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4SFDHxitoA-5"
   },
   "source": [
    "# Install libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9EQh61dvDCp0"
   },
   "outputs": [],
   "source": [
    "!pip install nltk spacy scikit-learn gensim matplotlib seaborn pandas tqdm flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1nUMIbUoLl-"
   },
   "source": [
    "# Thu morning 1 (NN on tf-idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mND6VjdqgoWB"
   },
   "source": [
    "In this session we will continue with building simple neural networks.\n",
    "We will use the more sophisticated features, and rely on previously established intuition about building the NNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwxYyCso5IFa"
   },
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoQhDMqOrtHU"
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wajOn8m5Oq8"
   },
   "source": [
    "## 2. Fetch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5xybPcrrcfd"
   },
   "outputs": [],
   "source": [
    "fetch_20newsgroups().target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NL8T5Kb3r-nW"
   },
   "source": [
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\n",
    "\n",
    "We strip the headers and footers, as those can make the task easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_1c1ZY5sDwM"
   },
   "outputs": [],
   "source": [
    "# Load the 20 Newsgroups dataset\n",
    "categories = ['rec.sport.hockey', 'sci.electronics', 'comp.graphics']\n",
    "SEED = 64\n",
    "train_data = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=SEED)\n",
    "test_data = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LLNn0H4FsPBl"
   },
   "outputs": [],
   "source": [
    "# inspect the data structure\n",
    "train_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gwj8tT7usQyl"
   },
   "outputs": [],
   "source": [
    "# inspect the data input elements, target, target_names, number of elements... (5 min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bAf5J1P6Z0Qk"
   },
   "outputs": [],
   "source": [
    "# after add the `remove=('headers', 'footers', 'quotes')` argument to the `fetch_20newsgroups` call and repeat\n",
    "# what changed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rP4-PTgK5jbN"
   },
   "source": [
    "## 3. Preprocess and inspect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KOXWnZ-vH2m"
   },
   "outputs": [],
   "source": [
    "# Iterate over the train_data and test_data\n",
    "for d in [train_data, test_data]:\n",
    "\n",
    "  # Remove leading and trailing whitespace, tab,\n",
    "  # and new line characters from each data point\n",
    "  d.data = [s.strip(' \\n\\t\\r') for s in d.data]\n",
    "\n",
    "  # Get the indices of data points with non-empty content\n",
    "  ok_idx = [i for i, s in enumerate(d.data) if len(s) > 0]\n",
    "\n",
    "  # Filter the data and target lists to keep only non-empty data points\n",
    "  d.data = [d.data[i] for i in ok_idx]\n",
    "  d.target = [d.target[i] for i in ok_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmPukYChsebf"
   },
   "outputs": [],
   "source": [
    "# inspect the data labels elements\n",
    "#train_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3wtbKp2csr-M"
   },
   "outputs": [],
   "source": [
    "len(train_data.data), len(test_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNJ87Mmksy3e"
   },
   "outputs": [],
   "source": [
    "sample = train_data.data[24]\n",
    "sample_label = train_data.target[24]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aveCoShCs0wU"
   },
   "outputs": [],
   "source": [
    "print(sample)\n",
    "print(f'label={sample_label}, ({categories[sample_label]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0floa6Fxw_J"
   },
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VUz-3pSW0C5c"
   },
   "outputs": [],
   "source": [
    "# Convert the text data to TF-IDF vectors, max 10 words\n",
    "vectorizer = TfidfVectorizer(max_features=10)\n",
    "\n",
    "# `object.fit` followed by processing by `object.transform` or the\n",
    "# joined `object.fit_transform` is the\n",
    "train_vectors = vectorizer.fit_transform(train_data.data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9yGUw5L0dm8"
   },
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(len(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zUgPgyEZ2E1"
   },
   "outputs": [],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_BaP1Jz43RR6"
   },
   "outputs": [],
   "source": [
    "# the vectors are stored in sparse format:\n",
    "train_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_0wi9r33wkt"
   },
   "outputs": [],
   "source": [
    "# to get the dense representation we need to convert them to an array:\n",
    "train_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZaVyRRK0T7k"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 3))\n",
    "plt.plot(vectorizer.get_feature_names_out(), train_vectors.toarray()[0])\n",
    "plt.plot(vectorizer.get_feature_names_out(), train_vectors.toarray()[1])\n",
    "plt.plot(vectorizer.get_feature_names_out(), train_vectors.toarray()[2])\n",
    "plt.xticks(rotation=45, horizontalalignment='right');\n",
    "plt.ylim(0,1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jf4GAKPHtZSz"
   },
   "outputs": [],
   "source": [
    "# Convert the text data to TF-IDF vectors, all words, all data\n",
    "vectorizer = ?\n",
    "train_vectors = ?\n",
    "test_vectors = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KiuwscRaaYnh"
   },
   "outputs": [],
   "source": [
    "# what's the difference in processing train_vectors and test vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqzoAV_9cH4T"
   },
   "outputs": [],
   "source": [
    "train_vectors.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IQTU4UaUuawJ"
   },
   "outputs": [],
   "source": [
    "af = train_vectors.toarray().flatten()\n",
    "# plot only present words (tfidf>0)\n",
    "aff = af[af>0]\n",
    "plt.hist(aff, 100, log=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4Zi-tyR5tNX"
   },
   "source": [
    "## 4. Create and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W3bn4Nn9a9vg"
   },
   "outputs": [],
   "source": [
    "# complete the code and train the model (20 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5oyrbUfDpqQX"
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the model hyperparameters\n",
    "input_dim = train_vectors.shape[1]\n",
    "hidden_dim = 256\n",
    "output_dim = len(categories)\n",
    "\n",
    "# Create the model instance\n",
    "model = ?\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = ?\n",
    "criterion = ?\n",
    "\n",
    "# Set the device (GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move the model and data to the device\n",
    "model = model.to(device)\n",
    "train_vectors_t = torch.tensor(train_vectors.toarray(), dtype=torch.float).to(device)\n",
    "train_labels_t = torch.tensor(train_data.target, dtype=torch.long).to(device)\n",
    "test_vectors_t = torch.tensor(test_vectors.toarray(), dtype=torch.float).to(device)\n",
    "test_labels_t = torch.tensor(test_data.target, dtype=torch.long).to(device)\n",
    "\n",
    "# Training loop\n",
    "def train(model, optimizer, criterion):\n",
    "    # set model to train mode\n",
    "    ?\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(train_vectors_t)\n",
    "    loss = criterion(predictions, train_labels_t)\n",
    "\n",
    "    # perform backpropagation\n",
    "    ?\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluation loop\n",
    "def evaluate(model):\n",
    "    # set model to evaluation mode\n",
    "    ?\n",
    "    with torch.no_grad():\n",
    "        predictions = model(test_vectors_t)\n",
    "        predicted_labels = torch.argmax(predictions, dim=1)\n",
    "        accuracy = torch.sum(predicted_labels == test_labels_t).item() / len(test_labels_t)\n",
    "    return accuracy\n",
    "\n",
    "# Training and evaluation loop\n",
    "NUM_EPOCHS = 100\n",
    "accuracy_arr = []\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "    train(model, optimizer, criterion)\n",
    "    accuracy = evaluate(model)\n",
    "    #print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    accuracy_arr.append(accuracy)\n",
    "\n",
    "# plot the accuracy evolution\n",
    "?\n",
    "print(max(accuracy_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66KqcGtlenZo"
   },
   "outputs": [],
   "source": [
    "# What do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-NUgYrD6Dfg"
   },
   "source": [
    "## 5. Exercise 1h + 30min discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO89XTV16grj"
   },
   "source": [
    "Work in 3 groups:\n",
    " - present final group results in the end (10 min per group):\n",
    " - half time through - share code of your intermediate results (on Zoom) so that other groups use it for final results:\n",
    "\n",
    "1. Optimize the model architecture to improve performance.\n",
    "2. Study dependence of performance & training time on number of tf-idf features\n",
    "3. Study relevant performance metrics. Make evaluation code for both training and the test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtwfrYsDoKVX"
   },
   "source": [
    "# Thu morning 2: RNN on embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wj0VGjy38y_d"
   },
   "source": [
    "## 1. Imports/utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_lEH0fEE5Q9"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as gd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vinmxOLyyTS2"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nI-xF3SBy9Om"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1DUCo_g2x8j"
   },
   "outputs": [],
   "source": [
    "def load_pckl(file_name, path=None):\n",
    "    if path is not None:\n",
    "        file_name = os.path.join(path, file_name)\n",
    "\n",
    "    with open(file_name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_pckl(d, file_name, pr=None, path=None):\n",
    "    if path is not None:\n",
    "        file_name = os.path.join(path, file_name)\n",
    "\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(d, f, protocol=pr if pr is not None else pickle.DEFAULT_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4PpKyLF7F9Fo"
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(input_string):\n",
    "    # Use regular expression to remove all punctuation characters\n",
    "    return re.sub(r'[^\\w\\s]', '', input_string)  # everything which is not a word (\\w) or space (\\s) -> empty string ('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XX6Kx_zwBb8e"
   },
   "source": [
    "## 2. Load and preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZW6zkt3LB47r"
   },
   "outputs": [],
   "source": [
    "# Load the 20 Newsgroups dataset\n",
    "categories = ['rec.sport.hockey', 'sci.electronics', 'comp.graphics']\n",
    "SEED = 64\n",
    "train_data = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=SEED, remove=('headers', 'footers', 'quotes'))\n",
    "test_data = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=SEED, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7HsPnlUEB47t"
   },
   "outputs": [],
   "source": [
    "# Iterate over the train_data and test_data\n",
    "for d in [train_data, test_data]:\n",
    "\n",
    "  # Remove leading and trailing whitespace, tab,\n",
    "  # and new line characters from each data point\n",
    "  d.data = [s.strip(' \\n\\t\\r') for s in d.data]\n",
    "\n",
    "  # Get the indices of data points with non-empty content\n",
    "  ok_idx = [i for i, s in enumerate(d.data) if len(s) > 0]\n",
    "\n",
    "  # Filter the data and target lists to keep only non-empty data points\n",
    "  d.data = [d.data[i] for i in ok_idx]\n",
    "  d.target = [d.target[i] for i in ok_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EV15Ei_70NH0"
   },
   "source": [
    "## 3. Explore Word2Vec (Homework)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sO_Dqnvf_3UT"
   },
   "source": [
    "Similar to the tiny dataset example explore similar words and word arythmetic.\n",
    "\n",
    "Use your imagination, e.g. snake-long+short, or brick-hard+soft, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAafXxdv_XNE"
   },
   "source": [
    "### 1. Tiny dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74dRLkXQ-iux"
   },
   "outputs": [],
   "source": [
    "# Preprocess the corpus (replace with your own preprocessing steps)\n",
    "corpus = [\"I love to eat pizza\", \"I hate Mondays\", \"Pizza is delicious\", \"I enjoy playing tennis\"]\n",
    "\n",
    "# Tokenize the corpus\n",
    "tokenized_corpus = [sentence.lower().split() for sentence in corpus]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9RvdcdY-kxd"
   },
   "outputs": [],
   "source": [
    "# Find similar words\n",
    "similar_words = model.wv.most_similar(\"pizza\")\n",
    "print(\"Similar words to 'pizza':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(word, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbIZeU2e-qkU"
   },
   "outputs": [],
   "source": [
    "# Perform word embeddings arithmetic\n",
    "result = model.wv.most_similar(positive=[\"tennis\", \"hate\"], negative=[\"love\"])  # \"tenis\" - \"love\" + \"hate\"\n",
    "print(\"Word embeddings arithmetic:\")\n",
    "for word, similarity in result:\n",
    "    print(word, similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rQIG8CgBG5q"
   },
   "source": [
    "As you see - on tiny dataset the vectors don't make much sense.\n",
    "You are encouraged to explore the embedding vectors built based on the bigger datasets in the nxt 2 sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oC5fQSXhJwMv"
   },
   "source": [
    "### 2. Bigger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCwp9tAdE3m1"
   },
   "outputs": [],
   "source": [
    "# Preprocess the corpus (replace with your own preprocessing steps)\n",
    "corpus = train_data.data\n",
    "\n",
    "# Tokenize the corpus\n",
    "tokenized_corpus = [remove_punctuation(sentence.lower()).split() for sentence in corpus]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(tokenized_corpus, vector_size=300, window=5, min_count=1, workers=4, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRjoibFZFBsd"
   },
   "outputs": [],
   "source": [
    "word = \"network\"\n",
    "similar_words = model.wv.most_similar(word, topn=20)\n",
    "print(f\"Similar words to '{word}':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(word, similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqACxydJCsKR"
   },
   "source": [
    "Try removing the stopwords and re-fitting the model (see sections below hoe to do it)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JMEoreu_hly"
   },
   "source": [
    "### 3. 300k words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoP9nc63DKUA"
   },
   "source": [
    "Here a pretrained model on many news articles is downloaded (Downloading it can take a while!). Feature vectors are of length 300, total 300k words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RhiLvBooJzcy"
   },
   "outputs": [],
   "source": [
    "# Download the word2vec-google-news-300 model\n",
    "model_gn300 = gd.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9kilDS5znQ2q"
   },
   "outputs": [],
   "source": [
    "model_gn300.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZI_Dau6jJ254"
   },
   "outputs": [],
   "source": [
    "word = \"car\"\n",
    "similar_words = model_gn300.most_similar(word)\n",
    "print(f\"Similar words to '{word}':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(word, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IklZGfZ1Keq5"
   },
   "outputs": [],
   "source": [
    "word = \"god\"\n",
    "similar_words = model_gn300.most_similar(word)\n",
    "print(f\"Similar words to '{word}':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(word, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "taBzRcUYEqSA"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Find similar words\n",
    "similar_words = model_gn300.most_similar(\"python\")\n",
    "print(\"Similar words to 'pizza':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(word, similarity)\n",
    "\n",
    "# Perform word embeddings arithmetic\n",
    "result = model_gn300.most_similar(positive=[\"python\", \"young\"], negative=[\"old\"])\n",
    "print(\"Word embeddings arithmetic:\")\n",
    "for word, similarity in result:\n",
    "    print(word, similarity)\n",
    "\n",
    "# Visualize word embeddings using t-SNE (replace with your own visualization code)\n",
    "# ...\n",
    "\n",
    "# Additional tasks and experiments with word embeddings\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHqmaxTw7Myf"
   },
   "source": [
    "## 4. Preparing embedding dataset (not run in the class - time-consuming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apoYWHguBV9C"
   },
   "source": [
    "### 0. Embedding utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BHATKtd8xc_"
   },
   "source": [
    "First we create word embeddigns with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4THcnUGGahs"
   },
   "outputs": [],
   "source": [
    "# Download (takes a while!) the word2vec-google-news-300 model\n",
    "model_gn300 = gd.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pq-cVuxRxMKJ"
   },
   "outputs": [],
   "source": [
    "def get_vec(model, word):\n",
    "  # convert word to vector\n",
    "  # Check if the model has an index for the word, of so get vector,\n",
    "  # otherwise return None\n",
    "  return model.get_vector(word) if model.has_index_for(word) else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f32YKff7x4Ie"
   },
   "outputs": [],
   "source": [
    "# Function to convert text to word vectors\n",
    "# using a specified word embedding model\n",
    "# (default - word2vec-google-news-300 model, Download takes a while!\n",
    "# you can also try yours)\n",
    "def convert_text_to_vecs(text, model=model_gn300):\n",
    "  # Tokenize the input text into individual words\n",
    "  words = nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "  # Get word vectors for each word in the text using the specified word embedding model\n",
    "  wvs = [get_vec(model, word) for word in words]\n",
    "\n",
    "  # Filter out None values (word vectors that couldn't be found in the model)\n",
    "  wvs = [v for v in wvs if v is not None]\n",
    "\n",
    "  # Convert the list of word vectors to a NumPy array\n",
    "  wvs = np.array(wvs)\n",
    "\n",
    "  # Return the array of word vectors for the input text\n",
    "  return wvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eceVSYzTA6uQ"
   },
   "source": [
    "Few helper functions to convert input dataset, and to save whole dataset to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGUcVK3Fx_rW"
   },
   "outputs": [],
   "source": [
    "def convert_corpus_to_vecs(corpus, convert_corpus_to_vecs_fn):\n",
    "  # for each text in the corpus - vectorize it\n",
    "  # using list comprehension. Use tqdm to display progress\n",
    "  return [convert_corpus_to_vecs_fn(text) for text in tqdm.auto.tqdm(corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EolcP7t417Fy"
   },
   "outputs": [],
   "source": [
    "def prepare_embedding_dataset(tra_input, tra_labels,\n",
    "                              val_input, val_labels,\n",
    "                              filename,\n",
    "                              convert_text_to_vecs_fn):\n",
    "  # Prepare and save an embedding dataset for training and validation.\n",
    "  print('embedding training data:')\n",
    "  tra_data_vecs = convert_corpus_to_vecs(tra_input, convert_text_to_vecs_fn)\n",
    "\n",
    "  print('embedding validation data:')\n",
    "  val_data_vecs = convert_corpus_to_vecs(val_input, convert_text_to_vecs_fn)\n",
    "\n",
    "  # remove empty elements\n",
    "  ok_idx = [i for i, s in enumerate(tra_data_vecs) if len(s)>0]\n",
    "  tra_data_vecs = [tra_data_vecs[i] for i in ok_idx]\n",
    "  tra_labels = [tra_labels[i] for i in ok_idx]\n",
    "\n",
    "  ok_idx = [i for i, s in enumerate(val_data_vecs) if len(s)>0]\n",
    "  val_data_vecs = [val_data_vecs[i] for i in ok_idx]\n",
    "  val_labels = [val_labels[i] for i in ok_idx]\n",
    "\n",
    "  print(f'preparing and saving dataset to: {filename}')\n",
    "\n",
    "  # Create a dataset dictionary containing training and validation data and labels\n",
    "  dataset = {\n",
    "    'tra_data': tra_data_vecs,\n",
    "    'tra_labels': tra_labels,\n",
    "    'val_data': val_data_vecs,\n",
    "    'val_labels': val_labels,\n",
    "  }\n",
    "\n",
    "  # Save the dataset as a pickle file\n",
    "  save_pckl(dataset, filename, path='./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6B65X6TPzvgC"
   },
   "source": [
    "GloVe and RoBERTa embeddings (with flair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-PYAb2tDtKO"
   },
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings, DocumentPoolEmbeddings, TransformerWordEmbeddings\n",
    "from flair.data import Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukDDr4132om0"
   },
   "outputs": [],
   "source": [
    "# Download models for word embeddings with GloVe and RoBERTa\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "roberta_embedding = TransformerWordEmbeddings('roberta-base')\n",
    "\n",
    "# Create a DocumentPoolEmbeddings object (optional but helpful)\n",
    "document_embeddings_glove = DocumentPoolEmbeddings([glove_embedding])\n",
    "document_embeddings_roberta = DocumentPoolEmbeddings([roberta_embedding])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uA60Vu8F1giW"
   },
   "outputs": [],
   "source": [
    "def convert_text_to_embedding(text, doc_embedding_model):\n",
    "  # Create a Flair Sentence object\n",
    "  sentence = Sentence(text)\n",
    "\n",
    "  # Embed the sentence\n",
    "  try:\n",
    "    doc_embedding_model.embed(sentence)\n",
    "  except Exception as e:\n",
    "    # if model can't convert a text - print it, before raising the exception\n",
    "    print('failed text:', text)\n",
    "    raise e\n",
    "\n",
    "  embeddings = [token.embedding.cpu().numpy() for token in sentence]\n",
    "  return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNWc1s7g6BYB"
   },
   "outputs": [],
   "source": [
    "def convert_text_to_glove(text):\n",
    "  return convert_text_to_embedding(text, document_embeddings_glove)\n",
    "\n",
    "def convert_text_to_roberta(text):\n",
    "  return convert_text_to_embedding(text, document_embeddings_roberta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bi1QNGL5BNZS"
   },
   "source": [
    "### 1. Create word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gtQxEiXzxxgQ"
   },
   "outputs": [],
   "source": [
    "prepare_embedding_dataset(train_data.data, train_data.target,\n",
    "                          test_data.data, test_data.target,\n",
    "                          filename='dataset_20newsgroups_3_cat.pckl',\n",
    "                          convert_text_to_vecs_fn=convert_text_to_vecs\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8Gw4tVw5yZS"
   },
   "outputs": [],
   "source": [
    "!mv dataset_20newsgroups_3_cat.pckl \"/content/drive/MyDrive/Colab Data/NLP_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccbxW2q4BgA0"
   },
   "source": [
    "### 2. Create GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_nuPY_S1NxX"
   },
   "outputs": [],
   "source": [
    "prepare_embedding_dataset(train_data.data, train_data.target,\n",
    "                          test_data.data, test_data.target,\n",
    "                          filename='dataset_20newsgroups_3_cat_gl.pckl',\n",
    "                          convert_text_to_vecs_fn=convert_text_to_glove\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KzmZdxTQ6u3X"
   },
   "outputs": [],
   "source": [
    "!mv dataset_20newsgroups_3_cat_gl.pckl \"/content/drive/MyDrive/Colab Data/NLP_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsTsJdNMBlhL"
   },
   "source": [
    "### 3. Create RoBERTa embeddings (run with a GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18U8TklyByWl"
   },
   "outputs": [],
   "source": [
    "prepare_embedding_dataset(train_data.data, train_data.target,\n",
    "                          test_data.data, test_data.target,\n",
    "                          filename='dataset_20newsgroups_3_cat_rb.pckl',\n",
    "                          convert_text_to_vecs_fn=convert_text_to_roberta\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mqQkRqACkH3"
   },
   "outputs": [],
   "source": [
    "!mv dataset_20newsgroups_3_cat_rb.pckl \"/content/drive/MyDrive/Colab Data/NLP_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NiyDVri7Wt6"
   },
   "source": [
    "### 4. Inspect prepared datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-dlLdY6dQEM"
   },
   "outputs": [],
   "source": [
    "dataset_20newsgroups_3_cat = load_pckl('dataset_20newsgroups_3_cat.pckl', '/content/drive/MyDrive/NLP_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wes5HklPddf0"
   },
   "outputs": [],
   "source": [
    "tra_data = dataset_20newsgroups_3_cat['tra_data']\n",
    "tra_labels = dataset_20newsgroups_3_cat['tra_labels']\n",
    "val_data = dataset_20newsgroups_3_cat['val_data']\n",
    "val_labels = dataset_20newsgroups_3_cat['val_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vdkC_4W3doQM"
   },
   "outputs": [],
   "source": [
    "len(tra_data), len(tra_labels), len(val_data), len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUY8vO-7d2cj"
   },
   "outputs": [],
   "source": [
    "tra_data[200].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CirttWtLCH3T"
   },
   "source": [
    "## 5. LSTM Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYzup5MWG714"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggt9UedMCXGE"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mTTIDjjCay0"
   },
   "outputs": [],
   "source": [
    "def load_pckl(file_name, path=None):\n",
    "    if path is not None:\n",
    "        file_name = os.path.join(path, file_name)\n",
    "\n",
    "    with open(file_name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_pckl(d, file_name, pr=None, path=None):\n",
    "    if path is not None:\n",
    "        file_name = os.path.join(path, file_name)\n",
    "\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(d, f, protocol=pr if pr is not None else pickle.DEFAULT_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sjh8Uw6GD2fn"
   },
   "outputs": [],
   "source": [
    "ds_name = 'dataset_20newsgroups_3_cat.pckl'\n",
    "\n",
    "\n",
    "# Load the embedded 20 Newsgroups dataset\n",
    "dataset = load_pckl(ds_name,\n",
    "                    '/content/drive/MyDrive/NLP_data')\n",
    "tra_data = dataset['tra_data']\n",
    "tra_labels = dataset['tra_labels']\n",
    "val_data = dataset['val_data']\n",
    "val_labels = dataset['val_labels']\n",
    "\n",
    "ok_idx = [i for i, s in enumerate(tra_data) if len(s)>0]\n",
    "tra_data = [tra_data[i] for i in ok_idx]\n",
    "tra_labels = [tra_labels[i] for i in ok_idx]\n",
    "\n",
    "ok_idx = [i for i, s in enumerate(val_data) if len(s)>0]\n",
    "val_data = [val_data[i] for i in ok_idx]\n",
    "val_labels = [val_labels[i] for i in ok_idx]\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNzFkb8sb09p"
   },
   "outputs": [],
   "source": [
    "# inspect preprocessing function and ensure you understand each line (10 min)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "batch_size = 50\n",
    "\n",
    "# make preprocessing function converting data to torch tensors\n",
    "def preprocess(data, labels):\n",
    "    # Make random crops of the sequences up to max_len length,\n",
    "    # Pad short sequences to ensure consistent sequence length\n",
    "    max_len = 128  # Maximum sequence length\n",
    "    lens = [len(d) for d in data]  # Get the length of each data point\n",
    "\n",
    "    # Calculate random offsets for padding\n",
    "    ofs = [np.random.randint(0, max(1, len_i - max_len)) for len_i in lens]\n",
    "\n",
    "    # Apply padding based on offsets\n",
    "    data = [d[o:o + max_len] for d, o in zip(data, ofs)]\n",
    "    max_len = max([len(d) for d in data])  # Update maximum sequence length after padding\n",
    "\n",
    "    # Pad sequences with zeros to match the maximum sequence length\n",
    "    # pad_width contains size of padding  left and righ in each dimension\n",
    "    data_padded = [np.pad(d, pad_width=((0, max_len - len(d)),\n",
    "                                        (0, 0))) for d in data]\n",
    "\n",
    "    # Convert data and labels to NumPy arrays\n",
    "    data_padded = np.array(data_padded)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Convert NumPy arrays to PyTorch tensors\n",
    "    # and move them to the specified device (e.g., GPU)\n",
    "    data_t = torch.tensor(data_padded, dtype=torch.float32).to(device)\n",
    "    labels_t = torch.tensor(labels, dtype=torch.int64).to(device)\n",
    "\n",
    "    return data_t, labels_t\n",
    "\n",
    "# make data loader\n",
    "\n",
    "train_loader = DataLoader(list(zip(tra_data, tra_labels)),\n",
    "                          batch_size=batch_size, shuffle=True,\n",
    "                          collate_fn=lambda x: preprocess(*zip(*x)),\n",
    "                          drop_last=True)\n",
    "\n",
    "test_loader = DataLoader(list(zip(val_data, val_labels)),\n",
    "                         batch_size=89, shuffle=False,  # 89*13==len(val_data)\n",
    "                         collate_fn=lambda x: preprocess(*zip(*x)),\n",
    "                         drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZo4-qLuEvKh"
   },
   "outputs": [],
   "source": [
    "class ClassificationRNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super(ClassificationRNN, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embedding_dim)\n",
    "\n",
    "        if type(hidden_dim) == int:\n",
    "          hidden_dim = [hidden_dim]\n",
    "\n",
    "        self.rnn = []\n",
    "        for i, hidd_d in enumerate(hidden_dim):\n",
    "          prev_d = embedding_dim if i == 0 else hidden_dim[i-1]\n",
    "          rnn = nn.LSTM(prev_d, hidd_d, batch_first=True)  # BSC\n",
    "\n",
    "          # most important line:\n",
    "          self.add_module(f'lstm_{i}', rnn)\n",
    "          self.rnn.append(rnn)\n",
    "\n",
    "        self.fc = nn.Linear(hidd_d, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply embedding layer and relu to the input\n",
    "        embedded = ?\n",
    "\n",
    "        # apply each rnn layer. tip: what does each layer return?\n",
    "        for rnn in self.rnn:\n",
    "          ?? = ?\n",
    "\n",
    "        output = embedded\n",
    "        rnn_out = hidden_hn[0]  # hn\n",
    "        return self.fc(rnn_out)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize the model\n",
    "input_dim = tra_data[0].shape[1]\n",
    "embedding_dim = 256\n",
    "hidden_dim = [128, 16]  # you can specify a list of number of units in sequential LSTM layers\n",
    "output_dim = 3\n",
    "\n",
    "model = ClassificationRNN(input_dim, embedding_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "# Define the sparse cross-entropy loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.005)\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 200\n",
    "\n",
    "num_warmup = 30                          # warm-up epochs\n",
    "\n",
    "# 1. linear warm-up from 0 → base lr\n",
    "warmup_sched = LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=0.0001,    # start at 0 × base_lr\n",
    "    end_factor=1.0,\n",
    "    total_iters=num_warmup\n",
    ")\n",
    "\n",
    "# 2. cosine decay from base lr → 0\n",
    "cosine_sched = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=n_epochs - num_warmup,\n",
    "    eta_min=0.0\n",
    ")\n",
    "\n",
    "# 3. chain them\n",
    "scheduler = SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[warmup_sched, cosine_sched],\n",
    "    milestones=[num_warmup]\n",
    ")\n",
    "\n",
    "\n",
    "tra_loss_hist = []\n",
    "val_loss_hist = []\n",
    "val_acc_hist = []\n",
    "lrs = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.\n",
    "    valid_loss = 0.\n",
    "\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        data, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # perform a training update:\n",
    "        ?\n",
    "\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    tra_loss_hist.append(train_loss)\n",
    "\n",
    "    scheduler.step()\n",
    "    lrs.append(scheduler.get_last_lr())\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = []\n",
    "        for batch in test_loader:\n",
    "            data, labels = batch\n",
    "            output = model(data)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            pred_class = torch.argmax(output, dim=1)\n",
    "            corr = pred_class == labels\n",
    "            correct.append(corr.detach().cpu().numpy())\n",
    "\n",
    "        valid_loss /= len(test_loader)\n",
    "        correct = np.concatenate(correct)\n",
    "        accuracy = np.mean(correct)\n",
    "        print(f\"{epoch}:\\t Test loss: {valid_loss}; accuracy: {accuracy}\")\n",
    "\n",
    "        val_loss_hist.append(valid_loss)\n",
    "        val_acc_hist.append(accuracy)\n",
    "\n",
    "vall_acc_max = np.max(val_acc_hist)\n",
    "vall_acc_max_eopch = np.argmax(val_acc_hist)\n",
    "\n",
    "print(f'Best val accuracy: {vall_acc_max:.3} @ epoch {vall_acc_max_eopch}')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plt.plot(lrs)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MdKYzdosJsXv"
   },
   "outputs": [],
   "source": [
    "# plot loss and accuracy on 2 subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.plot(tra_loss_hist, label='train')\n",
    "ax1.plot(val_loss_hist, label='test')\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(val_acc_hist)\n",
    "ax2.set_xlabel('epoch')\n",
    "ax2.set_ylabel('accuracy')\n",
    "plt.show()\n",
    "\n",
    "print(f'best validation accuracy: {np.max(val_acc_hist)} @ epoch {np.argmax(val_acc_hist)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ay-9UVS3I8AL"
   },
   "source": [
    "## 6. Exercise 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWVq4Yp1I_v4"
   },
   "source": [
    "Work in 3 groups, use the 3 datasets.\n",
    "Optimize the model architecture: # layers, units per layer (hidden_dim) and the embedding_dim to improve validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQ7YqgUjJpgW"
   },
   "source": [
    "## 7. Tip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIHJx96bJpgX"
   },
   "source": [
    "Same Flair interface as for Glove and Roberta, can be used to concatenate embeddings from several models, including your own ones. E.g., given your model file 'my_model.pt', you can create document embedder:\n",
    "\n",
    "```\n",
    "my_model = torch.load('my_model.pt')\n",
    "document_embeddings_my_model = DocumentPoolEmbeddings([my_model])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfKeQJujMSO2"
   },
   "source": [
    "alternatively - you can combine the Glove embeddings and your emebedding, or the roberta ones:\n",
    "```\n",
    "document_embeddings_stacked = DocumentPoolEmbeddings([my_model, glove_embedding])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7xmKKXAoQI3"
   },
   "source": [
    "# Thu evening-Friday morning (Transfer learning for PoS tagging/NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYu4_7i4Jf-c"
   },
   "source": [
    "### 0. Libs and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQ53du1av8Fz"
   },
   "outputs": [],
   "source": [
    "!pip install transformers[torch] datasets evaluate seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBhg36Fnv2kf"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import datasets\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from transformers import AutoModelForTokenClassification, AutoModel\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import evaluate\n",
    "\n",
    "import torch.cuda\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zxEZiBeEERL"
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fm9QHx-NJmmX"
   },
   "source": [
    "### 1. load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0bUkYgBJ0Um"
   },
   "source": [
    "Dataset info: https://huggingface.co/datasets/conll2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orVNobEcEGZ4"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"conll2003\", num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0_1NCHncz-k"
   },
   "outputs": [],
   "source": [
    "# inspect the dataset object (10 min):\n",
    "# 1. length of tra & test parts\n",
    "# 2. column names\n",
    "# 3. features\n",
    "# ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3h-wslc3dFz9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiuq90UihzC_"
   },
   "source": [
    "https://cs.nyu.edu/~grishman/jet/guide/PennPOS.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8u1gyjwpLr1F"
   },
   "outputs": [],
   "source": [
    "pos_tags_info = dataset['train'].features['pos_tags'].feature\n",
    "class_names = pos_tags_info.names\n",
    "\n",
    "class_idx_to_class_name = dict(enumerate(class_names))\n",
    "class_name_to_class_idx = {v:k for k, v in class_idx_to_class_name.items()}\n",
    "n_classes = len(class_names)\n",
    "\n",
    "class_types = list(set(class_names))  # get unique\n",
    "\n",
    "class_idx_to_class_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyiJ5yoqKBbp"
   },
   "source": [
    "### 2. Load tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hA6M92lHKKOZ"
   },
   "source": [
    "In the transformer library for most models you have the associated tokenizer.\n",
    "\n",
    "The methods AutoTokenizer and AutoModel instantiate a model of proper type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F3V-fo2rEzE_"
   },
   "outputs": [],
   "source": [
    "mod_name = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1VHg7j0EuuG"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(mod_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smgi6zkuKljj"
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(\"Decoding is going the other way around: from vocabulary indices, we want to get a string. This can be done with the decode() method as follows\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3lEZAZnKprO"
   },
   "outputs": [],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "br_d55KVfCSf"
   },
   "outputs": [],
   "source": [
    "decoded = tokenizer.decode(token_ids)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GS3C7m7ZdREb"
   },
   "outputs": [],
   "source": [
    "# play with tokenizer (10 min). How is the splitting performed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxjDZDRcdY5i"
   },
   "outputs": [],
   "source": [
    "# discuss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FE1arrrK2hc"
   },
   "source": [
    "### 3. Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgRxk1DyEqWB"
   },
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(mod_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1OkVnb0FVmJ"
   },
   "outputs": [],
   "source": [
    "example = dataset[\"test\"][560]\n",
    "example_txt = example['tokens']\n",
    "print(example_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WlyC6OnMFtv6"
   },
   "outputs": [],
   "source": [
    "example_tokens_ids = tokenizer(example_txt, is_split_into_words=True, return_tensors='pt')\n",
    "example_tokens_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jb1tAI3EHHba"
   },
   "outputs": [],
   "source": [
    "np_ids = example_tokens_ids['input_ids'].numpy()[0]\n",
    "tokens_r = tokenizer.convert_ids_to_tokens(np_ids)\n",
    "text_r = tokenizer.convert_tokens_to_string(tokens_r)\n",
    "text_r\n",
    "#tokens_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ATNKII15F77m"
   },
   "outputs": [],
   "source": [
    "res = model(**example_tokens_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5G8II8vGPNI"
   },
   "outputs": [],
   "source": [
    "for k, v in res.items():\n",
    "  print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7UzACcKNwM0"
   },
   "source": [
    "So.. What kind of model is this 'bert-base-uncased'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWmQ4jXjPw0H"
   },
   "source": [
    "Lets use it for transfer learning - we use it's features to do token classification. For this we create a model of type `AutoModelForTokenClassification`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Os3DeFqgIDAb"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(mod_name, num_labels=n_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxkHinpiIFbX"
   },
   "outputs": [],
   "source": [
    "res = model(**example_tokens_ids.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TOI-9lyIHQG"
   },
   "outputs": [],
   "source": [
    "for k, v in res.items():\n",
    "  print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j__GxfhGQabN"
   },
   "source": [
    "### 4. Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPUkNon5RQZy"
   },
   "source": [
    "we can use the tokenizer to covert text into token-ids. but there will be more of them than words. Our lables - are word based. We need thus to generate proper label - per token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0LurjzhZPeAZ"
   },
   "outputs": [],
   "source": [
    "# study the function (10 min)\n",
    "def tokenize_ner(examples, print_info=False):\n",
    "  # Extract word tokens and NER labels from examples\n",
    "  batch_words = examples['tokens']  # these are word tokens\n",
    "  batch_ner_tags = examples['pos_tags']  # examples['ner_tags']  # these are NER-labels per word\n",
    "\n",
    "  batch_tokens = tokenizer(batch_words, is_split_into_words=True, truncation=True)\n",
    "  batch_labels = []\n",
    "\n",
    "  LBL_IGN = -100  # label for tokens to be ignored during training\n",
    "\n",
    "  # Iterate through each example in the batch\n",
    "  for sample_idx, sample_ner_tags in enumerate(batch_ner_tags):\n",
    "    sample_labels = []  # To store NER labels for tokens in the current example\n",
    "\n",
    "    word_idxs = batch_tokens.word_ids(batch_index=sample_idx)\n",
    "    if print_info:\n",
    "      sample_token_ids = batch_tokens['input_ids'][sample_idx]\n",
    "      print(word_idxs, sample_ner_tags, tokenizer.convert_ids_to_tokens(sample_token_ids))\n",
    "\n",
    "    for word_idx in word_idxs:\n",
    "      if word_idx is None:  # Ignore tokens that are not related to real words\n",
    "        sample_labels.append(LBL_IGN)\n",
    "      else:\n",
    "        token_label = sample_ner_tags[word_idx]\n",
    "        sample_labels.append(token_label)  # Store the NER label for the token\n",
    "\n",
    "    # Add the NER labels for the current example to the batch_labels list\n",
    "    batch_labels.append(sample_labels)\n",
    "\n",
    "  # Add the batch_labels to the tokenized batch\n",
    "  batch_tokens['labels'] = batch_labels\n",
    "\n",
    "  return batch_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAVoGp8mS0RE"
   },
   "source": [
    "Lets inspect what and how does it do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQRFc-1ZP9uK"
   },
   "outputs": [],
   "source": [
    "res = tokenize_ner(dataset['train'][560:564], print_info=True)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOMzChvMXU0c"
   },
   "outputs": [],
   "source": [
    "for token, token_id, label in zip(tokenizer.convert_ids_to_tokens(res['input_ids'][0]), res['input_ids'][0], res['labels'][0]):\n",
    "  print(f'{token:<15} {token_id:<10} {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9NLOrvpY1pG"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_ner, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdRKBBXcZH8L"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets['train'][560]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whKgeLteb5aN"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yaqitZaPb5h3"
   },
   "outputs": [],
   "source": [
    "res = model(input_ids=torch.tensor(tokenized_datasets['train'][560:561]['input_ids']).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGulWhZmb5h4"
   },
   "outputs": [],
   "source": [
    "for k, v in res.items():\n",
    "  print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2Ghd82sUDwC"
   },
   "source": [
    "Lastly we need a collator.\n",
    "\n",
    "It ensures that tokenized inputs, labels, and other relevant data are properly formatted and batched together during training. This is particularly useful for tasks like Named Entity Recognition (NER) and part-of-speech tagging where token-level labels need to be aligned with input tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzf9Cq8rZ_j3"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JD6h9ObGZ01o"
   },
   "source": [
    "### 5. Performnce evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80W52h6danh4"
   },
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"poseval\")  # seqeval for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vm2gaNMIaniZ"
   },
   "outputs": [],
   "source": [
    "# Define a function to compute evaluation metrics for a given batch of predictions and labels.\n",
    "# The function takes eval_pred as input, which contains batch_logits and batch_labels.\n",
    "def compute_metrics(eval_pred, print_info=False):\n",
    "    # Extract batch_logits and batch_labels from eval_pred.\n",
    "    batch_logits, batch_labels = eval_pred\n",
    "\n",
    "    # Compute batch_predictions by selecting the class with the highest probability.\n",
    "    batch_predictions = np.argmax(batch_logits, axis=-1)\n",
    "\n",
    "    # Initialize lists to store filtered predictions and labels for each sample in the batch.\n",
    "    filtered_hr_batch_predictions = []\n",
    "    filtered_hr_batch_labels = []\n",
    "\n",
    "    # Iterate over samples in the batch to filter out padding tokens (-100).\n",
    "    for sample_prediction, sample_label in zip(batch_predictions, batch_labels):\n",
    "        filtered_hr_sample_predictions = []\n",
    "        filtered_hr_sample_labels = []\n",
    "\n",
    "        # Iterate over predictions and labels in each sample.\n",
    "        for prediction, label in zip(sample_prediction, sample_label):\n",
    "            # Check if the label is not a padding token (-100).\n",
    "            if label != -100:\n",
    "                # Convert prediction and label indices to class names using class_idx_to_class_name mapping.\n",
    "                filtered_hr_sample_predictions.append(class_idx_to_class_name[prediction])\n",
    "                filtered_hr_sample_labels.append(class_idx_to_class_name[label])\n",
    "\n",
    "        # Append filtered predictions and labels for the sample to the batch lists.\n",
    "        filtered_hr_batch_predictions.append(filtered_hr_sample_predictions)\n",
    "        filtered_hr_batch_labels.append(filtered_hr_sample_labels)\n",
    "\n",
    "    # Compute evaluation metrics using the filtered predictions and labels.\n",
    "    metric_res = metric.compute(predictions=filtered_hr_batch_predictions,\n",
    "                                references=filtered_hr_batch_labels)\n",
    "\n",
    "    # Optionally print information about filtered predictions, labels, and metric results.\n",
    "    if print_info:\n",
    "        print(filtered_hr_batch_predictions)\n",
    "        print(filtered_hr_batch_labels)\n",
    "        print(metric_res)\n",
    "\n",
    "    # Create a dictionary to store computed metrics.\n",
    "    all_metrics = {k: v for k, v in metric_res.items() if type(v) is not dict}\n",
    "    all_metrics = {**all_metrics, **metric_res['weighted avg']}\n",
    "\n",
    "    # Compute and add F1 scores for specific class types to the metrics dictionary.\n",
    "    for ct in class_types:\n",
    "        v = metric_res.get(ct, None)\n",
    "        all_metrics[ct + '_f1'] = v['f1-score'] if v is not None else 0.\n",
    "\n",
    "    # Return the computed evaluation metrics.\n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npn5MKzrlZ0e"
   },
   "outputs": [],
   "source": [
    "ds_test = tokenized_datasets['test']\n",
    "\n",
    "res = model(input_ids=torch.tensor(ds_test['input_ids'][:1]).to(device),\n",
    "            attention_mask=torch.tensor(ds_test['attention_mask'][:1]).to(device)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0Iqc1y6mMjn"
   },
   "outputs": [],
   "source": [
    "# don't forget to copy data to cpu-accessibel memory and convert to NumPy\n",
    "pred = res.logits.detach().cpu().numpy()\n",
    "lbl = tokenized_datasets['test']['labels'][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zUi4XrjhmZrk"
   },
   "outputs": [],
   "source": [
    "metrics_res = compute_metrics((pred, lbl), print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iqQUWG81piaU"
   },
   "outputs": [],
   "source": [
    "metrics_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLF6WmeeWTxW"
   },
   "source": [
    "Since the last model layer doing the 47-way classification is not yet trained - the model spits out rubbish, and the scores are 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4NUQNQuWfAC"
   },
   "source": [
    "### 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Vo2DcBkania"
   },
   "outputs": [],
   "source": [
    "# rm -rf ner logs_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOyWBrYOr8JT"
   },
   "outputs": [],
   "source": [
    "rm -rf pos logs_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zgrt5Y8Oania"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"pos\",\n",
    "                                  num_train_epochs=10,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  save_strategy=\"epoch\",\n",
    "\n",
    "                                  per_device_train_batch_size=16,  # batch size per device during training\n",
    "                                  per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "                                  warmup_steps=250,                # number of warmup steps for learning rate scheduler\n",
    "                                  weight_decay=0.01,               # strength of weight decay\n",
    "                                  logging_dir='./logs_pos',        # directory for storing logs\n",
    "                                  logging_steps=10,\n",
    "                                  # optim=\"adafactor\"\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glhIGAlvania"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    "    report_to='wandb',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xC9QaanQc1DG"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idpSRht2anic"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9HTQQrTbxFX4"
   },
   "outputs": [],
   "source": [
    "def plot_hist(log_hist):\n",
    "  \"\"\"\n",
    "  Helper function to aggregate and visualize training history\n",
    "  from trainers's logs\n",
    "  \"\"\"\n",
    "  last_loss = 0\n",
    "  s = 4\n",
    "  sfx = 'eval_'\n",
    "  e_loss_name = sfx+'loss'\n",
    "  sfx_skip = ['_per_second', 'runtime', 'epoch', 'step', 'confusion_matrix']\n",
    "\n",
    "  steps = []\n",
    "  loss = []\n",
    "\n",
    "  e_steps = []\n",
    "  e_loss = []\n",
    "  e_mtr = {}\n",
    "\n",
    "  e_cms = []\n",
    "\n",
    "  for el in log_hist:\n",
    "    if e_loss_name in el:\n",
    "      for k, v in el.items():\n",
    "        if any([s in k for s in sfx_skip]):\n",
    "          continue\n",
    "\n",
    "\n",
    "        if k == e_loss_name:\n",
    "          e_loss.append(v)\n",
    "        else:\n",
    "          if k not in e_mtr:\n",
    "            e_mtr[k] = []\n",
    "          e_mtr[k].append(v)\n",
    "\n",
    "      e_steps.append(el['step'])\n",
    "      if 'confusion_matrix' in el:\n",
    "        e_cms.append(el['confusion_matrix'])\n",
    "\n",
    "    else:\n",
    "      if 'loss' in el:\n",
    "        steps.append(el['step'])\n",
    "        loss.append(el['loss'])\n",
    "\n",
    "  n_fig = len(e_mtr)+1\n",
    "\n",
    "  fix, ax = plt.subplots(1, n_fig, figsize=(s*n_fig, s*1))\n",
    "  if n_fig < 2:\n",
    "    ax = [ax]\n",
    "  \n",
    "  ax[0].plot(steps, loss, alpha=0.5, label='tra')\n",
    "  ax[0].plot(e_steps, e_loss, alpha=0.5, label='val')\n",
    "  ax[0].set_xlabel('steps')\n",
    "  ax[0].set_title('loss')\n",
    "  ax[0].legend()\n",
    "\n",
    "  for idx, (lbl, vals) in enumerate(e_mtr.items()):\n",
    "    i = idx+1\n",
    "\n",
    "    #print(lbl, vals, e_steps)\n",
    "    ax[i].plot(e_steps, vals, alpha=0.5, label='val')\n",
    "    ax[i].set_xlabel('steps')\n",
    "    ax[i].set_title(lbl)\n",
    "\n",
    "    #ax[i].legend()\n",
    "\n",
    "  plt.show()\n",
    "  plt.close()\n",
    "\n",
    "  for i, cm in enumerate(e_cms):\n",
    "    plt.matshow(cm, cmap=plt.cm.Blues, alpha=0.3)\n",
    "    plt.title(e_steps[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "joL2BVTJeIcJ"
   },
   "outputs": [],
   "source": [
    "# summarize the steps in a flowchart on a whiteboard (15 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2ut-GVBk7ID"
   },
   "outputs": [],
   "source": [
    "plot_hist(trainer.state.log_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUUPCgbXXF9F"
   },
   "source": [
    "### 7. Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4FJNA65eo_8"
   },
   "source": [
    "Try modifying the code to do NER tag identification"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "4SFDHxitoA-5",
    "p1nUMIbUoLl-",
    "AtwfrYsDoKVX",
    "EV15Ei_70NH0",
    "sHqmaxTw7Myf",
    "E7xmKKXAoQI3"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
